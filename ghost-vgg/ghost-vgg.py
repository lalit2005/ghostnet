# -*- coding: utf-8 -*-
"""ghost_vgg_16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pgPr_mUS-xCwVEakB-bG2XgvM27CHIGC
"""

import torch
import torch.nn as nn
import math

class GhostModule(nn.Module):
    def __init__(self, in_channels, out_channels, primary_kernel_size=1, ratio=2, dw_kernel_size=3, stride=1):
        super(GhostModule, self).__init__()
        self.out_channels = out_channels
        self.ratio = ratio
        self.init_channels = math.ceil(out_channels / ratio)
        self.new_channels = self.init_channels * (self.ratio - 1)

        self.primary_conv = nn.Conv2d(
            in_channels,
            self.init_channels,
            kernel_size=primary_kernel_size,
            stride=stride,
            padding=(primary_kernel_size - 1) // 2,
            bias=False
        )

        self.cheap_operation = nn.Conv2d(
            self.init_channels,
            self.new_channels,
            kernel_size=dw_kernel_size,
            stride=1,
            padding=(dw_kernel_size - 1) // 2,
            groups=self.init_channels,
            bias=False
        )

    def forward(self, x):
        x1 = self.primary_conv(x)
        x2 = self.cheap_operation(x1)
        out = torch.cat([x1, x2], dim=1)
        return out[:, :self.out_channels, :, :]


class GhostBottleneck(nn.Module):
    """
    The Ghost Bottleneck block.

    This block is based on Figure 3 and the description
    in section 3.2.
    """
    def __init__(self, in_channels, hidden_dim, out_channels, dw_kernel_size, stride):
        super(GhostBottleneck, self).__init__()
        self.stride = stride

        # 1. Main Path
        # First GhostModule (expansion)
        self.ghost1 = GhostModule(
            in_channels,
            hidden_dim,
            primary_kernel_size=1,
            ratio=2,
            dw_kernel_size=dw_kernel_size
        )
        self.bn1 = nn.BatchNorm2d(hidden_dim)
        self.relu = nn.ReLU(inplace=True)

        # Optional Depthwise Conv for downsampling (if stride=2)
        if self.stride == 2:
            self.dwconv = nn.Conv2d(
                hidden_dim,
                hidden_dim,
                kernel_size=dw_kernel_size,
                stride=stride,
                padding=(dw_kernel_size - 1) // 2,
                groups=hidden_dim, # Depthwise
                bias=False
            )
            self.bn_dw = nn.BatchNorm2d(hidden_dim)

        # Second GhostModule (reduction)
        # Note: No ReLU after this module's BN
        self.ghost2 = GhostModule(
            hidden_dim,
            out_channels,
            primary_kernel_size=1,
            ratio=2,
            dw_kernel_size=dw_kernel_size
        )
        self.bn2 = nn.BatchNorm2d(out_channels)

        # 2. Shortcut Path
        # This path is used if stride=1 and in_c != out_c, or if stride=2
        if stride == 1 and in_channels == out_channels:
            self.shortcut = nn.Identity()
        else:
            shortcut_layers = []
            if stride == 2:
                shortcut_layers.append(nn.Conv2d(
                    in_channels,
                    in_channels,
                    kernel_size=dw_kernel_size,
                    stride=stride,
                    padding=(dw_kernel_size - 1) // 2,
                    groups=in_channels, # Depthwise
                    bias=False
                ))
                shortcut_layers.append(nn.BatchNorm2d(in_channels))

            shortcut_layers.append(nn.Conv2d(
                in_channels,
                out_channels,
                kernel_size=1,
                stride=1,
                padding=0,
                bias=False
            ))
            shortcut_layers.append(nn.BatchNorm2d(out_channels))

            self.shortcut = nn.Sequential(*shortcut_layers)

    def forward(self, x):
        identity = self.shortcut(x)

        x = self.ghost1(x)
        x = self.bn1(x)
        x = self.relu(x)

        if self.stride == 2:
            x = self.dwconv(x)
            x = self.bn_dw(x)

        x = self.ghost2(x)
        x = self.bn2(x)

        return x + identity


class GhostNet(nn.Module):
    def __init__(self, num_classes=1000, width_mult=1.0):
        super(GhostNet, self).__init__()
        self.width_mult = width_mult

        # Format: [expansion_size, out_channels, stride, dw_kernel_size]
        # We derive dw_kernel_size from the fact that all G-bnecks
        # use the same kernel size for cheap ops (paper uses 3x3 )
        # and strided blocks use a dw_conv of the same kernel size .
        # We will use 3x3 as the default.
        self.config = [
            # #exp, #out, stride, dw_kernel
            [16, 16, 1, 3],  # G-bneck, $112^2 \times 16$
            [48, 24, 2, 3],  # G-bneck, $56^2 \times 24$
            [72, 24, 1, 3],  # G-bneck, $56^2 \times 24$
            [72, 40, 2, 5],  # G-bneck, $28^2 \times 40$ (kernel 5x5)
            [120, 40, 1, 5], # G-bneck, $28^2 \times 40$ (kernel 5x5)
            [240, 80, 2, 3], # G-bneck, $14^2 \times 80$
            [200, 80, 1, 3], # G-bneck, $14^2 \times 80$
            [184, 80, 1, 3], # G-bneck, $14^2 \times 80$
            [184, 80, 1, 3], # G-bneck, $14^2 \times 80$
            [480, 112, 1, 3],# G-bneck, $14^2 \times 112$
            [672, 112, 1, 3],# G-bneck, $14^2 \times 112$
            [672, 160, 2, 5],# G-bneck, $7^2 \times 160$ (kernel 5x5)
            [960, 160, 1, 5],# G-bneck, $7^2 \times 160$ (kernel 5x5)
            [960, 160, 1, 5],# G-bneck, $7^2 \times 160$ (kernel 5x5)
            [960, 160, 1, 5],# G-bneck, $7^2 \times 160$ (kernel 5x5)
            [960, 160, 1, 5] # G-bneck, $7^2 \times 160$ (kernel 5x5)
        ]
        
        def _make_divisible(v, divisor=8):
            new_v = max(divisor, int(v + divisor / 2) // divisor * divisor)
            if new_v < 0.9 * v:
                new_v += divisor
            return new_v
        
        def _apply_width(v):
            return _make_divisible(v * self.width_mult)

        # --- Build Layers ---

        # 1. First layer (Conv2d 3x3) 
        self.in_channels = _apply_width(16)
        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=3, stride=2, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(self.in_channels)
        self.relu = nn.ReLU(inplace=True)

        # 2. Stack of Ghost Bottlenecks 
        self.bottlenecks = self.create_bottlenecks(_apply_width)

        # 3. Final layers
        # Conv2d 1x1
        self.out_channels = _apply_width(960)
        self.conv_head = nn.Conv2d(self.in_channels, self.out_channels, kernel_size=1, stride=1, padding=0, bias=False)
        self.bn_head = nn.BatchNorm2d(self.out_channels)
        
        # AvgPool 7x7
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        
        # Conv2d 1x1
        self.classifier_conv = nn.Conv2d(self.out_channels, 1280, kernel_size=1, stride=1, padding=0, bias=True)
        
        # FC
        self.fc = nn.Linear(1280, num_classes)
        
        # Initialize weights
        self._initialize_weights()

    def create_bottlenecks(self, _apply_width):
        """Builds the main stack of GhostBottlenecks."""
        layers = []
        for exp_size, out_c, stride, dw_kernel in self.config:
            hidden_dim = _apply_width(exp_size)
            out_channels = _apply_width(out_c)
            
            layers.append(GhostBottleneck(
                self.in_channels,
                hidden_dim,
                out_channels,
                dw_kernel_size=dw_kernel,
                stride=stride
            ))
            self.in_channels = out_channels # Update in_channels for next block
        return nn.Sequential(*layers)

    def forward(self, x):
        # First layer
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        
        # Bottlenecks
        x = self.bottlenecks(x)
        
        # Head
        x = self.conv_head(x)
        x = self.bn_head(x)
        x = self.relu(x)
        
        x = self.avgpool(x)
        
        x = self.classifier_conv(x)
        x = self.relu(x)
        
        # Flatten for FC
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        
        return x

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)

#-----------------------GhostNet Complete---------------------------------------------------------

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import time

# ==========================================
# 1. Standard VGG-16 for CIFAR-10
# ==========================================
class VGG16_CIFAR(nn.Module):
    def __init__(self, num_classes=10):
        super(VGG16_CIFAR, self).__init__()

        # Standard VGG-16 Configuration
        # M = MaxPool
        self.cfg = [
            64, 64, 'M',
            128, 128, 'M',
            256, 256, 256, 'M',
            512, 512, 512, 'M',
            512, 512, 512, 'M'
        ]

        self.features = self.make_layers(self.cfg)

        # Classifier for CIFAR-10
        self.classifier = nn.Sequential(
            nn.Dropout(),
            nn.Linear(512, 512),
            nn.ReLU(True),
            nn.Dropout(),
            nn.Linear(512, num_classes),
        )

    def make_layers(self, cfg):
        layers = []
        in_channels = 3
        for x in cfg:
            if x == 'M':
                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]
            else:
                # Standard Conv2d -> BN -> ReLU
                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),
                           nn.BatchNorm2d(x),
                           nn.ReLU(inplace=True)]
                in_channels = x
        return nn.Sequential(*layers)

    def forward(self, x):
        out = self.features(x)
        out = out.view(out.size(0), -1)
        out = self.classifier(out)
        return out

# ==========================================
# 2. Replacement Function
# ==========================================
def replace_conv_with_ghost(model, ratio=2, dw_kernel_size=3):
    """
    Replaces nn.Conv2d with existing GhostModule.
    Adapted to match your class signature:
    (in_channels, out_channels, primary_kernel_size, ratio, dw_kernel_size, stride)
    """
    for name, module in model.named_children():
        if isinstance(module, nn.Conv2d):
            # 1. Capture existing config
            in_channels = module.in_channels
            out_channels = module.out_channels
            kernel_size = module.kernel_size[0]
            stride = module.stride[0]

            # 2. Create GhostModule
            # We explicitly pass 'primary_kernel_size' as the original kernel size (usually 3)
            ghost_layer = GhostModule(
                in_channels=in_channels,
                out_channels=out_channels,
                primary_kernel_size=kernel_size,
                ratio=ratio,
                dw_kernel_size=dw_kernel_size,
                stride=stride
            )

            # 3. Swap it
            setattr(model, name, ghost_layer)

        else:
            # Recursive call
            replace_conv_with_ghost(module, ratio, dw_kernel_size)

# ==========================================
# 3. Training Loop
# ==========================================
def train_ghost_vgg():
    # Hyperparameters
    BATCH_SIZE = 128
    LR = 0.1
    EPOCHS = 200  # Set to 160+ for paper replication
    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

    print(f"Using device: {DEVICE}")

    # Data
    print("Preparing CIFAR-10 Data...")
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])

    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)

    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)

    # 1. Build Model
    print("\nBuilding Standard VGG-16...")
    net = VGG16_CIFAR()

    # Check Standard Params
    std_params = sum(p.numel() for p in net.parameters())
    print(f"Standard VGG Params: {std_params / 1e6:.2f}M")

    # 2. Swap Layers
    print("Swapping Conv2d with GhostModules...")
    replace_conv_with_ghost(net, ratio=2, dw_kernel_size=3)

    net = net.to(DEVICE)

    # Check Ghost Params
    ghost_params = sum(p.numel() for p in net.parameters())
    print(f"Ghost-VGG Params: {ghost_params / 1e6:.2f}M")
    print(f"Reduction: {100 * (1 - ghost_params/std_params):.2f}%")

    # 3. Setup Training
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)

    # 4. Train
    print("\nStarting Training...")
    start_time = time.time()

    for epoch in range(EPOCHS):
        net.train()
        train_loss = 0
        correct = 0
        total = 0

        for batch_idx, (inputs, targets) in enumerate(trainloader):
            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)
            optimizer.zero_grad()
            outputs = net(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

        # Validation
        net.eval()
        test_correct = 0
        test_total = 0
        with torch.no_grad():
            for inputs, targets in testloader:
                inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)
                outputs = net(inputs)
                _, predicted = outputs.max(1)
                test_total += targets.size(0)
                test_correct += predicted.eq(targets).sum().item()

        acc = 100. * test_correct / test_total
        print(f"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss/(batch_idx+1):.3f} | Test Acc: {acc:.2f}%")
        scheduler.step()

    print(f"Total time: {time.time() - start_time:.1f}s")

if __name__ == '__main__':
    # Ensure GhostModule is defined in the notebook before running this!
    try:
        GhostModule
        train_ghost_vgg()
    except NameError:
        print("Error: 'GhostModule' is not defined. Please run your GhostModule code cell first.")


# OUTPUT
# Using device: cuda
# Preparing CIFAR-10 Data...
#
# Building Standard VGG-16...
# Standard VGG Params: 14.99M
# Swapping Conv2d with GhostModules...
# Ghost-VGG Params: 7.65M
# Reduction: 48.97%
#
# Starting Training...
# Epoch 1/200 | Train Loss: 2.133 | Test Acc: 18.40%
# Epoch 2/200 | Train Loss: 1.897 | Test Acc: 32.51%
# Epoch 3/200 | Train Loss: 1.676 | Test Acc: 37.89%
# Epoch 4/200 | Train Loss: 1.401 | Test Acc: 48.61%
# Epoch 5/200 | Train Loss: 1.187 | Test Acc: 60.14%
# Epoch 6/200 | Train Loss: 1.019 | Test Acc: 66.60%
# Epoch 7/200 | Train Loss: 0.891 | Test Acc: 60.51%
# Epoch 8/200 | Train Loss: 0.827 | Test Acc: 66.29%
# Epoch 9/200 | Train Loss: 0.780 | Test Acc: 74.32%
# Epoch 10/200 | Train Loss: 0.736 | Test Acc: 76.27%
# Epoch 11/200 | Train Loss: 0.704 | Test Acc: 75.29%
# Epoch 12/200 | Train Loss: 0.675 | Test Acc: 71.13%
# Epoch 13/200 | Train Loss: 0.657 | Test Acc: 73.90%
# Epoch 14/200 | Train Loss: 0.641 | Test Acc: 80.37%
# Epoch 15/200 | Train Loss: 0.622 | Test Acc: 68.18%
# Epoch 16/200 | Train Loss: 0.604 | Test Acc: 76.06%
# Epoch 17/200 | Train Loss: 0.592 | Test Acc: 75.76%
# Epoch 18/200 | Train Loss: 0.588 | Test Acc: 78.55%
# Epoch 19/200 | Train Loss: 0.570 | Test Acc: 75.06%
# Epoch 20/200 | Train Loss: 0.565 | Test Acc: 74.89%
# Epoch 21/200 | Train Loss: 0.559 | Test Acc: 79.63%
# Epoch 22/200 | Train Loss: 0.557 | Test Acc: 76.92%
# Epoch 23/200 | Train Loss: 0.550 | Test Acc: 74.44%
# Epoch 24/200 | Train Loss: 0.551 | Test Acc: 76.81%
# Epoch 25/200 | Train Loss: 0.542 | Test Acc: 75.22%
# Epoch 26/200 | Train Loss: 0.531 | Test Acc: 76.81%
# Epoch 27/200 | Train Loss: 0.535 | Test Acc: 77.17%
# Epoch 28/200 | Train Loss: 0.523 | Test Acc: 76.40%
# Epoch 29/200 | Train Loss: 0.520 | Test Acc: 82.28%
# Epoch 30/200 | Train Loss: 0.516 | Test Acc: 77.92%
# Epoch 31/200 | Train Loss: 0.514 | Test Acc: 84.00%
# Epoch 32/200 | Train Loss: 0.506 | Test Acc: 79.15%
# Epoch 33/200 | Train Loss: 0.508 | Test Acc: 76.74%
# Epoch 34/200 | Train Loss: 0.496 | Test Acc: 82.15%
# Epoch 35/200 | Train Loss: 0.502 | Test Acc: 79.18%
# Epoch 36/200 | Train Loss: 0.494 | Test Acc: 80.89%
# Epoch 37/200 | Train Loss: 0.489 | Test Acc: 82.11%
# Epoch 38/200 | Train Loss: 0.489 | Test Acc: 81.12%
# Epoch 39/200 | Train Loss: 0.489 | Test Acc: 81.65%
# Epoch 40/200 | Train Loss: 0.484 | Test Acc: 77.01%
# Epoch 41/200 | Train Loss: 0.480 | Test Acc: 83.25%
# Epoch 42/200 | Train Loss: 0.474 | Test Acc: 76.90%
# Epoch 43/200 | Train Loss: 0.473 | Test Acc: 81.52%
# Epoch 44/200 | Train Loss: 0.473 | Test Acc: 79.67%
# Epoch 45/200 | Train Loss: 0.471 | Test Acc: 83.71%
# Epoch 46/200 | Train Loss: 0.460 | Test Acc: 81.53%
# Epoch 47/200 | Train Loss: 0.464 | Test Acc: 81.85%
# Epoch 48/200 | Train Loss: 0.462 | Test Acc: 83.89%
# Epoch 49/200 | Train Loss: 0.455 | Test Acc: 81.86%
# Epoch 50/200 | Train Loss: 0.456 | Test Acc: 80.78%
# Epoch 51/200 | Train Loss: 0.456 | Test Acc: 82.48%
# Epoch 52/200 | Train Loss: 0.449 | Test Acc: 84.23%
# Epoch 53/200 | Train Loss: 0.440 | Test Acc: 80.03%
# Epoch 54/200 | Train Loss: 0.441 | Test Acc: 80.07%
# Epoch 55/200 | Train Loss: 0.432 | Test Acc: 84.05%
# Epoch 56/200 | Train Loss: 0.443 | Test Acc: 71.68%
# Epoch 57/200 | Train Loss: 0.439 | Test Acc: 82.20%
# Epoch 58/200 | Train Loss: 0.433 | Test Acc: 80.46%
# Epoch 59/200 | Train Loss: 0.434 | Test Acc: 81.12%
# Epoch 60/200 | Train Loss: 0.426 | Test Acc: 83.77%
# Epoch 61/200 | Train Loss: 0.412 | Test Acc: 83.62%
# Epoch 62/200 | Train Loss: 0.421 | Test Acc: 79.49%
# Epoch 63/200 | Train Loss: 0.423 | Test Acc: 84.41%
# Epoch 64/200 | Train Loss: 0.415 | Test Acc: 83.58%
# Epoch 65/200 | Train Loss: 0.413 | Test Acc: 84.92%
# Epoch 66/200 | Train Loss: 0.414 | Test Acc: 81.70%
# Epoch 67/200 | Train Loss: 0.404 | Test Acc: 83.96%
# Epoch 68/200 | Train Loss: 0.402 | Test Acc: 82.89%
# Epoch 69/200 | Train Loss: 0.402 | Test Acc: 82.84%
# Epoch 70/200 | Train Loss: 0.397 | Test Acc: 84.04%
# Epoch 71/200 | Train Loss: 0.404 | Test Acc: 84.86%
# Epoch 72/200 | Train Loss: 0.390 | Test Acc: 83.04%
# Epoch 73/200 | Train Loss: 0.396 | Test Acc: 83.96%
# Epoch 74/200 | Train Loss: 0.390 | Test Acc: 81.06%
# Epoch 75/200 | Train Loss: 0.378 | Test Acc: 84.11%
# Epoch 76/200 | Train Loss: 0.383 | Test Acc: 84.98%
# Epoch 77/200 | Train Loss: 0.378 | Test Acc: 83.68%
# Epoch 78/200 | Train Loss: 0.376 | Test Acc: 82.12%
# Epoch 79/200 | Train Loss: 0.373 | Test Acc: 85.89%
# Epoch 80/200 | Train Loss: 0.374 | Test Acc: 83.99%
# Epoch 81/200 | Train Loss: 0.367 | Test Acc: 85.26%
# Epoch 82/200 | Train Loss: 0.366 | Test Acc: 84.11%
# Epoch 83/200 | Train Loss: 0.358 | Test Acc: 85.63%
# Epoch 84/200 | Train Loss: 0.360 | Test Acc: 85.58%
# Epoch 85/200 | Train Loss: 0.355 | Test Acc: 81.74%
# Epoch 86/200 | Train Loss: 0.356 | Test Acc: 83.53%
# Epoch 87/200 | Train Loss: 0.353 | Test Acc: 85.62%
# Epoch 88/200 | Train Loss: 0.350 | Test Acc: 85.79%
# Epoch 89/200 | Train Loss: 0.340 | Test Acc: 78.88%
# Epoch 90/200 | Train Loss: 0.336 | Test Acc: 85.05%
# Epoch 91/200 | Train Loss: 0.335 | Test Acc: 80.28%
# Epoch 92/200 | Train Loss: 0.336 | Test Acc: 83.90%
# Epoch 93/200 | Train Loss: 0.328 | Test Acc: 85.52%
# Epoch 94/200 | Train Loss: 0.326 | Test Acc: 83.09%
# Epoch 95/200 | Train Loss: 0.321 | Test Acc: 84.40%
# Epoch 96/200 | Train Loss: 0.321 | Test Acc: 81.99%
# Epoch 97/200 | Train Loss: 0.319 | Test Acc: 86.95%
# Epoch 98/200 | Train Loss: 0.318 | Test Acc: 83.68%
# Epoch 99/200 | Train Loss: 0.309 | Test Acc: 85.20%
# Epoch 100/200 | Train Loss: 0.310 | Test Acc: 84.66%
# Epoch 101/200 | Train Loss: 0.301 | Test Acc: 87.12%
# Epoch 102/200 | Train Loss: 0.304 | Test Acc: 86.34%
# Epoch 103/200 | Train Loss: 0.298 | Test Acc: 87.10%
# Epoch 104/200 | Train Loss: 0.295 | Test Acc: 81.63%
# Epoch 105/200 | Train Loss: 0.299 | Test Acc: 88.04%
# Epoch 106/200 | Train Loss: 0.281 | Test Acc: 88.80%
# Epoch 107/200 | Train Loss: 0.282 | Test Acc: 88.42%
# Epoch 108/200 | Train Loss: 0.279 | Test Acc: 87.95%
# Epoch 109/200 | Train Loss: 0.281 | Test Acc: 85.04%
# Epoch 110/200 | Train Loss: 0.273 | Test Acc: 83.29%
# Epoch 111/200 | Train Loss: 0.278 | Test Acc: 85.75%
# Epoch 112/200 | Train Loss: 0.263 | Test Acc: 86.31%
# Epoch 113/200 | Train Loss: 0.260 | Test Acc: 87.84%
# Epoch 114/200 | Train Loss: 0.257 | Test Acc: 86.45%
# Epoch 115/200 | Train Loss: 0.251 | Test Acc: 87.63%
# Epoch 116/200 | Train Loss: 0.252 | Test Acc: 86.08%
# Epoch 117/200 | Train Loss: 0.249 | Test Acc: 89.33%
# Epoch 118/200 | Train Loss: 0.243 | Test Acc: 84.43%
# Epoch 119/200 | Train Loss: 0.238 | Test Acc: 87.16%
# Epoch 120/200 | Train Loss: 0.237 | Test Acc: 88.80%
# Epoch 121/200 | Train Loss: 0.229 | Test Acc: 88.28%
# Epoch 122/200 | Train Loss: 0.224 | Test Acc: 87.52%
# Epoch 123/200 | Train Loss: 0.223 | Test Acc: 85.61%
# Epoch 124/200 | Train Loss: 0.218 | Test Acc: 89.00%
# Epoch 125/200 | Train Loss: 0.212 | Test Acc: 87.82%
# Epoch 126/200 | Train Loss: 0.218 | Test Acc: 87.34%
# Epoch 127/200 | Train Loss: 0.206 | Test Acc: 89.02%
# Epoch 128/200 | Train Loss: 0.200 | Test Acc: 87.68%
# Epoch 129/200 | Train Loss: 0.199 | Test Acc: 87.87%
# Epoch 130/200 | Train Loss: 0.194 | Test Acc: 86.83%
# Epoch 131/200 | Train Loss: 0.192 | Test Acc: 87.89%
# Epoch 132/200 | Train Loss: 0.186 | Test Acc: 88.73%
# Epoch 133/200 | Train Loss: 0.180 | Test Acc: 88.03%
# Epoch 134/200 | Train Loss: 0.176 | Test Acc: 89.07%
# Epoch 135/200 | Train Loss: 0.177 | Test Acc: 89.47%
# Epoch 136/200 | Train Loss: 0.168 | Test Acc: 89.57%
# Epoch 137/200 | Train Loss: 0.168 | Test Acc: 88.90%
# Epoch 138/200 | Train Loss: 0.154 | Test Acc: 87.72%
# Epoch 139/200 | Train Loss: 0.157 | Test Acc: 88.12%
# Epoch 140/200 | Train Loss: 0.151 | Test Acc: 89.58%
# Epoch 141/200 | Train Loss: 0.143 | Test Acc: 90.11%
# Epoch 142/200 | Train Loss: 0.146 | Test Acc: 89.99%
# Epoch 143/200 | Train Loss: 0.139 | Test Acc: 88.36%
# Epoch 144/200 | Train Loss: 0.137 | Test Acc: 90.14%
# Epoch 145/200 | Train Loss: 0.133 | Test Acc: 90.24%
# Epoch 146/200 | Train Loss: 0.128 | Test Acc: 89.60%
# Epoch 147/200 | Train Loss: 0.122 | Test Acc: 90.69%
# Epoch 148/200 | Train Loss: 0.123 | Test Acc: 89.46%
# Epoch 149/200 | Train Loss: 0.120 | Test Acc: 90.52%
# Epoch 150/200 | Train Loss: 0.113 | Test Acc: 89.35%
# Epoch 151/200 | Train Loss: 0.112 | Test Acc: 90.08%
# Epoch 152/200 | Train Loss: 0.102 | Test Acc: 90.40%
# Epoch 153/200 | Train Loss: 0.098 | Test Acc: 91.26%
# Epoch 154/200 | Train Loss: 0.096 | Test Acc: 91.41%
# Epoch 155/200 | Train Loss: 0.088 | Test Acc: 90.21%
# Epoch 156/200 | Train Loss: 0.087 | Test Acc: 91.11%
# Epoch 157/200 | Train Loss: 0.079 | Test Acc: 91.06%
# Epoch 158/200 | Train Loss: 0.080 | Test Acc: 91.41%
# Epoch 159/200 | Train Loss: 0.073 | Test Acc: 90.50%
# Epoch 160/200 | Train Loss: 0.067 | Test Acc: 91.13%
# Epoch 161/200 | Train Loss: 0.065 | Test Acc: 91.14%
# Epoch 162/200 | Train Loss: 0.060 | Test Acc: 91.27%
# Epoch 163/200 | Train Loss: 0.055 | Test Acc: 91.67%
# Epoch 164/200 | Train Loss: 0.056 | Test Acc: 91.63%
# Epoch 165/200 | Train Loss: 0.050 | Test Acc: 92.13%
# Epoch 166/200 | Train Loss: 0.048 | Test Acc: 91.92%
# Epoch 167/200 | Train Loss: 0.046 | Test Acc: 92.23%
# Epoch 168/200 | Train Loss: 0.039 | Test Acc: 92.38%
# Epoch 169/200 | Train Loss: 0.034 | Test Acc: 92.16%
# Epoch 170/200 | Train Loss: 0.034 | Test Acc: 92.28%
# Epoch 171/200 | Train Loss: 0.032 | Test Acc: 92.14%
# Epoch 172/200 | Train Loss: 0.025 | Test Acc: 92.56%
# Epoch 173/200 | Train Loss: 0.024 | Test Acc: 92.54%
# Epoch 174/200 | Train Loss: 0.022 | Test Acc: 92.66%
# Epoch 175/200 | Train Loss: 0.020 | Test Acc: 93.11%
# Epoch 176/200 | Train Loss: 0.019 | Test Acc: 92.62%
# Epoch 177/200 | Train Loss: 0.016 | Test Acc: 92.87%
# Epoch 178/200 | Train Loss: 0.015 | Test Acc: 92.81%
# Epoch 179/200 | Train Loss: 0.011 | Test Acc: 92.82%
# Epoch 180/200 | Train Loss: 0.012 | Test Acc: 93.10%
# Epoch 181/200 | Train Loss: 0.009 | Test Acc: 93.21%
# Epoch 182/200 | Train Loss: 0.009 | Test Acc: 93.15%
# Epoch 183/200 | Train Loss: 0.008 | Test Acc: 93.28%
# Epoch 184/200 | Train Loss: 0.007 | Test Acc: 93.22%
# Epoch 185/200 | Train Loss: 0.007 | Test Acc: 93.41%
# Epoch 186/200 | Train Loss: 0.005 | Test Acc: 93.45%
# Epoch 187/200 | Train Loss: 0.005 | Test Acc: 93.42%
# Epoch 188/200 | Train Loss: 0.005 | Test Acc: 93.49%
# Epoch 189/200 | Train Loss: 0.004 | Test Acc: 93.40%
# Epoch 190/200 | Train Loss: 0.004 | Test Acc: 93.50%
# Epoch 191/200 | Train Loss: 0.004 | Test Acc: 93.56%
# Epoch 192/200 | Train Loss: 0.004 | Test Acc: 93.57%
# Epoch 193/200 | Train Loss: 0.004 | Test Acc: 93.62%
# Epoch 194/200 | Train Loss: 0.003 | Test Acc: 93.53%
# Epoch 195/200 | Train Loss: 0.003 | Test Acc: 93.60%
# Epoch 196/200 | Train Loss: 0.003 | Test Acc: 93.54%
# Epoch 197/200 | Train Loss: 0.004 | Test Acc: 93.55%
# Epoch 198/200 | Train Loss: 0.003 | Test Acc: 93.63%
# Epoch 199/200 | Train Loss: 0.004 | Test Acc: 93.59%
# Epoch 200/200 | Train Loss: 0.003 | Test Acc: 93.60%
# Total time: 4834.1
