# -*- coding: utf-8 -*-
"""resnet after plot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r53wwSr9airRq1vZ5IwTwQD2J9eKp5zW
"""

import torch
import torch.nn as nn
import math
import torch.nn.functional as F
import torch.optim as optim
import time
import torchvision
import torchvision.transforms as transforms

# ==============================================================================
# 1. Ghost Module
# ==============================================================================
class GhostModule(nn.Module):
  def __init__(self, in_channels, out_channels, primary_kernel_size=1, ratio = 2, dw_kernel_size=3, stride=1):
    super(GhostModule, self).__init__()
    self.ratio = ratio
    self.out_channels = out_channels

    # M: Number of intrinsic channels (N / R, rounded up)
    self.init_channels = math.ceil(out_channels / ratio)
    # S: Number of cheap operation channels (M * (R - 1))
    self.new_channels = self.init_channels * (self.ratio - 1)

    # Primary Conv: Generates M intrinsic features
    self.primary_conv = nn.Conv2d(
       in_channels,
       self.init_channels,
       kernel_size = primary_kernel_size,
       stride = stride,
       padding = (primary_kernel_size - 1) // 2,
       bias = False
    )

    # Cheap Operations: Generates S ghost features from M intrinsic features
    self.cheap_operations = nn.Conv2d(
      in_channels = self.init_channels,
      out_channels = self.new_channels,
      kernel_size = dw_kernel_size,
      stride = 1, # Always stride 1 for cheap operations
      padding = (dw_kernel_size - 1) // 2,
      groups = self.init_channels, # Depth-wise convolution
      bias = False
    )

  def forward(self, x):
    intrinsic_features = self.primary_conv(x)
    cheap_features = self.cheap_operations(intrinsic_features)
    # Concatenate intrinsic and ghost features
    out = torch.cat([intrinsic_features, cheap_features], dim = 1)
    # Truncate to the desired output channels N
    return out[:, :self.out_channels, :, :]

# ==============================================================================
# 2. Ghost Bottleneck (The new building block)
# ==============================================================================

# Helper for Depthwise Separable Conv (if needed)
class GhostBottleneck_DW_Wrapper(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1):
        super(GhostBottleneck_DW_Wrapper, self).__init__()
        # Depthwise convolution
        self.depthwise = nn.Conv2d(
            in_channels,
            in_channels,
            kernel_size=kernel_size,
            stride=stride,
            padding=(kernel_size - 1) // 2,
            groups=in_channels,
            bias=False
        )
        self.bn = nn.BatchNorm2d(in_channels)

        # Pointwise convolution (1x1) to mix features
        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)

    def forward(self, x):
        out = self.depthwise(x)
        out = self.bn(out)
        out = F.relu(out)
        out = self.pointwise(out)
        out = self.bn2(out)
        return out


class GhostBottleneck(nn.Module):
    expansion = 1

    # in_planes: input channels, mid_planes: expansion channels, out_planes: output channels
    def __init__(self, in_planes, mid_planes, out_planes, stride, dw_kernel_size=3):
        super(GhostBottleneck, self).__init__()

        # 1. Ghost Module 1 (Expansion)
        self.ghost_module1 = GhostModule(
            in_channels=in_planes,
            out_channels=mid_planes,
            primary_kernel_size=1,
            stride=1 # GhostModule always has stride 1
        )
        self.bn1 = nn.BatchNorm2d(mid_planes)

        # 2. Depthwise Separable Convolution (Handles stride/downsampling)
        if stride > 1:
          # Use the DW wrapper for stride, or a simple DW conv + 1x1 conv layer
          self.dw_conv = GhostBottleneck_DW_Wrapper(
              in_channels=mid_planes,
              out_channels=mid_planes,
              kernel_size=dw_kernel_size,
              stride=stride
          )
        else:
          self.dw_conv = nn.Identity()

        # 3. Ghost Module 2 (Projection/Compression)
        self.ghost_module2 = GhostModule(
            in_channels=mid_planes,
            out_channels=out_planes,
            primary_kernel_size=1,
            stride=1
        )
        self.bn2 = nn.BatchNorm2d(out_planes)

        # Shortcut: 1x1 Conv with stride if dimensions mismatch
        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != out_planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_planes)
            )

    def forward(self, x):
        identity = x

        # 1. Expansion Ghost Module + Activation
        out = F.relu(self.bn1(self.ghost_module1(x)))

        # 2. Depthwise Conv (handles downsampling if stride > 1)
        out = self.dw_conv(out)

        # 3. Projection Ghost Module (no activation here)
        out = self.bn2(self.ghost_module2(out))

        # Add shortcut (Residual connection)
        out += self.shortcut(identity)

        return F.relu(out) # Final ReLU

# ==============================================================================
# 3. Ghost ResNet Architecture
# ==============================================================================
class ResNet_GB(nn.Module):
    def __init__(self, block, num_blocks, num_classes = 10):
        super(ResNet_GB, self).__init__()
        self.in_planes = 16 # Initial channel count

        # Initial stem layer (Standard Conv - kept standard for performance)
        self.conv1 = nn.Conv2d(3, 16, kernel_size = 3, stride = 1, padding = 1, bias = False)
        self.bn1 = nn.BatchNorm2d(16)

        # Stage 1: 16 channels in/out, Stride 1
        self.layer1 = self.make_layer(block, 16, 16, 16, num_blocks[0], stride = 1)
        # Stage 2: 16 in, 32 out, Stride 2
        self.layer2 = self.make_layer(block, 16, 32, 32, num_blocks[1], stride = 2)
        # Stage 3: 32 in, 64 out, Stride 2
        self.layer3 = self.make_layer(block, 32, 64, 64, num_blocks[2], stride = 2)

        # Final linear layer
        self.linear = nn.Linear(64, num_classes) # Final output channels is 64

    def make_layer(self, block, in_planes, mid_planes, out_planes, num_blocks, stride):
        strides = [stride] + [1]*(num_blocks - 1)
        layers = []
        for s in strides:
            # The Ghost Bottleneck needs in_planes, mid_planes (expansion), and out_planes
            # This logic ensures the first block uses self.in_planes (from the previous stage)
            # and subsequent blocks use out_planes (from the current block's output)
            current_in_planes = self.in_planes if layers == [] else out_planes
            layers.append(block(current_in_planes, mid_planes, out_planes, s))

            # Update the channel count for the next stage's first block
            self.in_planes = out_planes

        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = F.avg_pool2d(out, out.size()[3]) # Global Avg Pooling
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out

def GhostResNet56():
    # Uses the Ghost Bottleneck in a 56-layer like structure: [9, 9, 9] blocks per stage
    return ResNet_GB(GhostBottleneck, [9, 9, 9])

# ==============================================================================
# 4. Training Setup and Execution
# ==============================================================================

print("======================================")
print("GhostNet-56 (using Ghost Bottlenecks)")
print("======================================")
model = GhostResNet56()

# Check for CUDA availability
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# Hyperparameters
optimizer = optim.SGD(model.parameters(), lr = 0.1, momentum = 0.9, weight_decay = 1e-4)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 200)
criterion = nn.CrossEntropyLoss()

# Data Transformations
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

batch_size = 128

# Data Loaders
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)

print(f"Data loaders created. Device: {device}")

# Training Function
def train(epoch):
    model.train()
    train_loss = 0
    correct = 0
    total = 0
    for batch_id, (inputs, targets) in enumerate(trainloader):
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()

# Testing Function
def test(epoch):
    model.eval()
    test_loss = 0
    correct = 0
    total = 0
    with torch.no_grad():
        for batch_id, (inputs, targets) in enumerate(testloader):
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)

            test_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

    acc = 100. * correct / total
    print(f'Epoch {epoch} Test Acc: {acc:.3f}%')
    return acc

# Main Loop
print(f"Starting training for 200 epochs...")
best_acc = 0.0

start_time = time.time()
for epoch in range(200):
    train(epoch)
    acc = test(epoch)
    scheduler.step()

    if acc > best_acc:
        best_acc = acc
        torch.save(model.state_dict(), 'ghost_resnet56_best.pth')
end_time = time.time()

print(f"\nFinal Best Accuracy: {best_acc:.3f}%")
print(f"Total training time: {(end_time - start_time) / 60:.2f} minutes")

import pandas as pd
import re
import matplotlib.pyplot as plt

# The log data from your prompt
log_data = """
======================================
GhostNet-56 (using Ghost Bottlenecks)
======================================


Data loaders created. Device: cuda:0
Starting training for 200 epochs...
Epoch 0 Test Acc: 32.270%
Epoch 1 Test Acc: 50.210%
Epoch 2 Test Acc: 58.380%
Epoch 3 Test Acc: 56.360%
Epoch 4 Test Acc: 70.880%
Epoch 5 Test Acc: 70.000%
Epoch 6 Test Acc: 74.100%
Epoch 7 Test Acc: 74.540%
Epoch 8 Test Acc: 77.700%
Epoch 9 Test Acc: 73.430%
Epoch 10 Test Acc: 79.480%
Epoch 11 Test Acc: 69.980%
Epoch 12 Test Acc: 79.810%
Epoch 13 Test Acc: 76.080%
Epoch 14 Test Acc: 81.340%
Epoch 15 Test Acc: 78.800%
Epoch 16 Test Acc: 80.110%
Epoch 17 Test Acc: 79.590%
Epoch 18 Test Acc: 78.850%
Epoch 19 Test Acc: 81.510%
Epoch 20 Test Acc: 82.710%
Epoch 21 Test Acc: 81.690%
Epoch 22 Test Acc: 81.330%
Epoch 23 Test Acc: 77.360%
Epoch 24 Test Acc: 79.090%
Epoch 25 Test Acc: 84.210%
Epoch 26 Test Acc: 80.890%
Epoch 27 Test Acc: 83.230%
Epoch 28 Test Acc: 82.420%
Epoch 29 Test Acc: 82.150%
Epoch 30 Test Acc: 83.720%
Epoch 31 Test Acc: 82.420%
Epoch 32 Test Acc: 78.080%
Epoch 33 Test Acc: 83.700%
Epoch 34 Test Acc: 82.190%
Epoch 35 Test Acc: 84.140%
Epoch 36 Test Acc: 83.710%
Epoch 37 Test Acc: 82.350%
Epoch 38 Test Acc: 83.290%
Epoch 39 Test Acc: 82.600%
Epoch 40 Test Acc: 84.620%
Epoch 41 Test Acc: 86.250%
Epoch 42 Test Acc: 84.400%
Epoch 43 Test Acc: 81.950%
Epoch 44 Test Acc: 85.630%
Epoch 45 Test Acc: 85.110%
Epoch 46 Test Acc: 83.020%
Epoch 47 Test Acc: 85.480%
Epoch 48 Test Acc: 86.100%
Epoch 49 Test Acc: 81.760%
Epoch 50 Test Acc: 86.380%
Epoch 51 Test Acc: 84.730%
Epoch 52 Test Acc: 82.640%
Epoch 53 Test Acc: 86.560%
Epoch 54 Test Acc: 82.190%
Epoch 55 Test Acc: 86.120%
Epoch 56 Test Acc: 85.180%
Epoch 57 Test Acc: 83.850%
Epoch 58 Test Acc: 86.110%
Epoch 59 Test Acc: 86.580%
Epoch 60 Test Acc: 85.160%
Epoch 61 Test Acc: 83.010%
Epoch 62 Test Acc: 85.440%
Epoch 63 Test Acc: 85.850%
Epoch 64 Test Acc: 83.630%
Epoch 65 Test Acc: 86.260%
Epoch 66 Test Acc: 85.700%
Epoch 67 Test Acc: 87.030%
Epoch 68 Test Acc: 86.490%
Epoch 69 Test Acc: 85.890%
Epoch 70 Test Acc: 86.220%
Epoch 71 Test Acc: 85.690%
Epoch 72 Test Acc: 86.730%
Epoch 73 Test Acc: 84.590%
Epoch 74 Test Acc: 83.110%
Epoch 75 Test Acc: 84.540%
Epoch 76 Test Acc: 86.620%
Epoch 77 Test Acc: 86.590%
Epoch 78 Test Acc: 87.050%
Epoch 79 Test Acc: 86.950%
Epoch 80 Test Acc: 87.260%
Epoch 81 Test Acc: 86.470%
Epoch 82 Test Acc: 86.870%
Epoch 83 Test Acc: 86.720%
Epoch 84 Test Acc: 87.550%
Epoch 85 Test Acc: 86.970%
Epoch 86 Test Acc: 86.070%
Epoch 87 Test Acc: 88.060%
Epoch 88 Test Acc: 85.670%
Epoch 89 Test Acc: 86.080%
Epoch 90 Test Acc: 88.140%
Epoch 91 Test Acc: 87.680%
Epoch 92 Test Acc: 85.680%
Epoch 93 Test Acc: 88.650%
Epoch 94 Test Acc: 87.140%
Epoch 95 Test Acc: 88.320%
Epoch 96 Test Acc: 88.470%
Epoch 97 Test Acc: 87.010%
Epoch 98 Test Acc: 87.190%
Epoch 99 Test Acc: 88.060%
Epoch 100 Test Acc: 87.970%
Epoch 101 Test Acc: 87.270%
Epoch 102 Test Acc: 87.330%
Epoch 103 Test Acc: 85.430%
Epoch 104 Test Acc: 88.240%
Epoch 105 Test Acc: 87.760%
Epoch 106 Test Acc: 88.140%
Epoch 107 Test Acc: 87.480%
Epoch 108 Test Acc: 88.390%
Epoch 109 Test Acc: 88.440%
Epoch 110 Test Acc: 87.920%
Epoch 111 Test Acc: 89.360%
Epoch 112 Test Acc: 88.640%
Epoch 113 Test Acc: 89.100%
Epoch 114 Test Acc: 88.510%
Epoch 115 Test Acc: 87.630%
Epoch 116 Test Acc: 88.980%
Epoch 117 Test Acc: 87.850%
Epoch 118 Test Acc: 89.460%
Epoch 119 Test Acc: 88.550%
Epoch 120 Test Acc: 88.560%
Epoch 121 Test Acc: 88.810%
Epoch 122 Test Acc: 88.950%
Epoch 123 Test Acc: 89.560%
Epoch 124 Test Acc: 89.150%
Epoch 125 Test Acc: 89.200%
Epoch 126 Test Acc: 88.830%
Epoch 127 Test Acc: 89.220%
Epoch 128 Test Acc: 89.260%
Epoch 129 Test Acc: 89.510%
Epoch 130 Test Acc: 88.930%
Epoch 131 Test Acc: 88.280%
Epoch 132 Test Acc: 89.290%
Epoch 133 Test Acc: 89.500%
Epoch 134 Test Acc: 89.620%
Epoch 135 Test Acc: 89.360%
Epoch 136 Test Acc: 90.040%
Epoch 137 Test Acc: 89.140%
Epoch 138 Test Acc: 90.220%
Epoch 139 Test Acc: 89.180%
Epoch 140 Test Acc: 89.140%
Epoch 141 Test Acc: 89.630%
Epoch 142 Test Acc: 89.820%
Epoch 143 Test Acc: 89.520%
Epoch 144 Test Acc: 89.840%
Epoch 145 Test Acc: 90.150%
Epoch 146 Test Acc: 90.380%
Epoch 147 Test Acc: 90.320%
Epoch 148 Test Acc: 90.230%
Epoch 149 Test Acc: 90.380%
Epoch 150 Test Acc: 89.790%
Epoch 151 Test Acc: 90.050%
Epoch 152 Test Acc: 90.270%
Epoch 153 Test Acc: 90.420%
Epoch 154 Test Acc: 90.270%
Epoch 155 Test Acc: 90.550%
Epoch 156 Test Acc: 90.260%
Epoch 157 Test Acc: 90.530%
Epoch 158 Test Acc: 90.180%
Epoch 159 Test Acc: 90.400%
Epoch 160 Test Acc: 90.620%
Epoch 161 Test Acc: 90.690%
Epoch 162 Test Acc: 90.790%
Epoch 163 Test Acc: 90.540%
Epoch 164 Test Acc: 90.700%
Epoch 165 Test Acc: 90.660%
Epoch 166 Test Acc: 90.540%
Epoch 167 Test Acc: 90.790%
Epoch 168 Test Acc: 90.920%
Epoch 169 Test Acc: 90.870%
Epoch 170 Test Acc: 90.750%
Epoch 171 Test Acc: 90.930%
Epoch 172 Test Acc: 90.740%
Epoch 173 Test Acc: 90.940%
Epoch 174 Test Acc: 91.220%
Epoch 175 Test Acc: 90.880%
Epoch 176 Test Acc: 91.010%
Epoch 177 Test Acc: 91.030%
Epoch 178 Test Acc: 90.980%
Epoch 179 Test Acc: 90.820%
Epoch 180 Test Acc: 91.300%
Epoch 181 Test Acc: 91.140%
Epoch 182 Test Acc: 91.080%
Epoch 183 Test Acc: 91.120%
Epoch 184 Test Acc: 91.070%
Epoch 185 Test Acc: 91.070%
Epoch 186 Test Acc: 91.080%
Epoch 187 Test Acc: 91.010%
Epoch 188 Test Acc: 91.200%
Epoch 189 Test Acc: 91.180%
Epoch 190 Test Acc: 91.090%
Epoch 191 Test Acc: 91.130%
Epoch 192 Test Acc: 91.130%
Epoch 193 Test Acc: 91.140%
Epoch 194 Test Acc: 91.120%
Epoch 195 Test Acc: 91.090%
Epoch 196 Test Acc: 91.150%
Epoch 197 Test Acc: 91.160%
Epoch 198 Test Acc: 91.060%
Epoch 199 Test Acc: 91.030%

Final Best Accuracy: 91.300%
Total training time: 125.87 minutes
"""

# Use regex to find all lines that contain "Epoch" and "Test Acc" and extract the numbers.
pattern = re.compile(r"Epoch\s*(\d+)\s*Test Acc:\s*([\d\.]+)%")
matches = pattern.findall(log_data)

# Convert matches to a DataFrame
data = [(int(epoch), float(acc)) for epoch, acc in matches]
df = pd.DataFrame(data, columns=['Epoch', 'Test Acc (%)'])

# Plotting the data
plt.figure(figsize=(12, 6))
plt.plot(df['Epoch'], df['Test Acc (%)'], marker='o', linestyle='-', color='b', markersize=3)
plt.title('GhostNet-56 Test Accuracy Over Epochs', fontsize=16)
plt.xlabel('Epoch', fontsize=14)
plt.ylabel('Test Accuracy (%)', fontsize=14)
plt.grid(True, linestyle='--', alpha=0.7)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
plt.tight_layout()

# Find the maximum accuracy point for annotation
max_acc = df['Test Acc (%)'].max()
max_epoch = df.loc[df['Test Acc (%)'].idxmax(), 'Epoch']

# Add an annotation for the maximum accuracy
plt.axhline(y=max_acc, color='r', linestyle='--', linewidth=1)
plt.plot(max_epoch, max_acc, 'ro') # Mark the max point
plt.annotate(
    f'Max Acc: {max_acc:.3f}% @ Epoch {max_epoch}',
    xy=(max_epoch, max_acc),
    xytext=(max_epoch + 10, max_acc - 5),
    arrowprops=dict(facecolor='black', shrink=0.05, width=1, headwidth=5),
    fontsize=12,
    color='r'
)

# Save the plot (saved as ghostnet_test_accuracy_vs_epoch.png)
plt.savefig('ghostnet_test_accuracy_vs_epoch.png')

