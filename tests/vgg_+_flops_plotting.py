# -*- coding: utf-8 -*-
"""vgg + flops plotting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1olA73D_k_J5sc-3Vx1Le3NtiE5dvIfaY
"""

pip install thop

import torch
import torch.nn as nn
import math
import torch.nn.functional as F
import torch.optim as optim
import time
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
from thop import profile # Import thop for FLOPs calculation

# ==============================================================================
# 1. Ghost Module (Kept the same)
# ==============================================================================
class GhostModule(nn.Module):
  def __init__(self, in_channels, out_channels, primary_kernel_size=3, ratio = 2, dw_kernel_size=3, stride=1):
    super(GhostModule, self).__init__()
    self.ratio = ratio
    self.out_channels = out_channels

    # M: Number of intrinsic channels (N / R, rounded up)
    self.init_channels = math.ceil(out_channels / ratio)
    # S: Number of cheap operation channels (M * (R - 1))
    self.new_channels = self.init_channels * (self.ratio - 1)

    # Primary Conv: Generates M intrinsic features
    self.primary_conv = nn.Conv2d(
       in_channels,
       self.init_channels,
       kernel_size = primary_kernel_size,
       stride = stride,
       padding = (primary_kernel_size - 1) // 2,
       bias = False
    )

    # Cheap Operations: Generates S ghost features from M intrinsic features
    self.cheap_operations = nn.Conv2d(
      in_channels = self.init_channels,
      out_channels = self.new_channels,
      kernel_size = dw_kernel_size,
      stride = 1, # Always stride 1 for cheap operations
      padding = (dw_kernel_size - 1) // 2,
      groups = self.init_channels, # Depth-wise convolution
      bias = False
    )

  def forward(self, x):
    intrinsic_features = self.primary_conv(x)
    cheap_features = self.cheap_operations(intrinsic_features)
    # Concatenate intrinsic and ghost features
    out = torch.cat([intrinsic_features, cheap_features], dim = 1)
    # Truncate to the desired output channels N
    return out[:, :self.out_channels, :, :]

# ==============================================================================
# 2. Ghost VGG Architecture (Kept the same)
# ==============================================================================

# VGG-16 configuration: List of output channels for each layer. 'M' denotes MaxPool.
cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M']

# Function to create the layers using GhostModule
def make_layers(cfg, in_channels=3, kernel_size=3):
    layers = []

    for v in cfg:
        if v == 'M':
            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]
        else:
            out_channels = v
            # Replace standard Conv2D with GhostModule
            layers += [
                GhostModule(
                    in_channels,
                    out_channels,
                    primary_kernel_size=kernel_size,
                    stride=1
                ),
                nn.BatchNorm2d(out_channels), # Add BN after GhostModule
                nn.ReLU(inplace=True)
            ]
            in_channels = out_channels

    return nn.Sequential(*layers)


class GhostVGG(nn.Module):
    def __init__(self, features, num_classes=10):
        super(GhostVGG, self).__init__()

        self.features = features

        # Classifier for CIFAR-10 (32x32 image -> 1x1 feature map after 5 MaxPools)
        self.classifier = nn.Sequential(
            nn.Linear(512 * 1 * 1, 4096),
            nn.ReLU(True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(True),
            nn.Dropout(),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        out = self.features(x)
        out = out.view(out.size(0), -1)
        out = self.classifier(out)
        return out

def GhostVGG16():
    return GhostVGG(make_layers(cfg))

# ==============================================================================
# 3. Model Instantiation and FLOPs Calculation
# ==============================================================================

print("======================================")
print("Ghost VGG-16 (Conv layers replaced with GhostModule)")
print("======================================")
model = GhostVGG16()

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# --- FLOPs and Params Calculation Segment ---
input_tensor = torch.randn(1, 3, 32, 32).to(device)
flops, params = profile(model, inputs=(input_tensor, ), verbose=False)

# Convert FLOPs to Giga-FLOPs (GFLOPs)
gflops = flops / 1e9

print(f"Model Parameters (M): {params / 1e6:.2f} M")
print(f"Model FLOPs (G): {gflops:.2f} G")
print("--------------------------------------")
# ---------------------------------------------

# Hyperparameters
optimizer = optim.SGD(model.parameters(), lr = 0.1, momentum = 0.9, weight_decay = 1e-4)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 200)
criterion = nn.CrossEntropyLoss()

# Data Transformations and Loaders (Kept the same)
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

batch_size = 128

trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)

print(f"Data loaders created. Device: {device}")

# --- Lists for tracking metrics ---
train_loss_history = []
test_acc_history = []
epochs_list = []
# ----------------------------------

# Training Function
def train(epoch):
    model.train()
    train_loss = 0
    correct = 0
    total = 0
    for batch_id, (inputs, targets) in enumerate(trainloader):
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()

    # Record average training loss for the epoch
    avg_train_loss = train_loss / len(trainloader)
    train_loss_history.append(avg_train_loss)


# Testing Function
def test(epoch):
    model.eval()
    test_loss = 0
    correct = 0
    total = 0
    with torch.no_grad():
        for batch_id, (inputs, targets) in enumerate(testloader):
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)

            test_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

    acc = 100. * correct / total
    print(f'Epoch {epoch+1:3d} | Test Acc: {acc:.3f}%')

    # Record accuracy and epoch number
    epochs_list.append(epoch + 1)
    test_acc_history.append(acc)
    return acc

# Main Loop
print(f"Starting training for 200 epochs...")
best_acc = 0.0

start_time = time.time()
for epoch in range(200):
    train(epoch)
    acc = test(epoch)
    scheduler.step()

    if acc > best_acc:
        best_acc = acc
        torch.save(model.state_dict(), 'ghost_vgg16_best.pth')
end_time = time.time()

print(f"\nFinal Best Accuracy: {best_acc:.3f}%")
print(f"Total training time: {(end_time - start_time) / 60:.2f} minutes")

# ==============================================================================
# 4. Plotting Segment
# ==============================================================================

print("\nGenerating Plots...")

plt.figure(figsize=(12, 5))

# --- Plot 1: Accuracy vs. Epoch ---
plt.subplot(1, 2, 1)
plt.plot(epochs_list, test_acc_history, label='Test Accuracy', color='blue')
plt.title('Test Accuracy vs. Epoch')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.grid(True)
plt.legend()

# --- Plot 2: Loss vs. Epoch ---
plt.subplot(1, 2, 2)
plt.plot(epochs_list, train_loss_history, label='Train Loss', color='red')
plt.title('Training Loss vs. Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss (Cross Entropy)')
plt.grid(True)
plt.legend()

plt.tight_layout()

# --- Plot 3: FLOPs and Accuracy (Single Point) ---
# Since FLOPs are constant, we plot the final best accuracy against the FLOPs value.
# This gives context for the model's efficiency.
plt.figure(figsize=(6, 5))
plt.scatter([gflops], [best_acc], color='green', s=200, label=f'Ghost VGG-16')
plt.title('Model Efficiency Trade-off')
plt.xlabel(f'FLOPs (G) - {gflops:.2f} G')
plt.ylabel('Best Test Accuracy (%)')
plt.text(gflops, best_acc + 0.5, f'{best_acc:.2f}%', ha='center', color='black')
plt.grid(True)
plt.legend()

plt.show()
print("Plots saved/displayed successfully.")

"""======================================
Ghost VGG-16 (Conv layers replaced with GhostModule)
======================================
Model Parameters (M): 26.31 M
Model FLOPs (G): 0.18 G
--------------------------------------
100%|██████████| 170M/170M [00:14<00:00, 11.8MB/s]
Data loaders created. Device: cuda:0
Starting training for 200 epochs...
Epoch   1 | Test Acc: 18.320%
Epoch   2 | Test Acc: 35.340%
Epoch   3 | Test Acc: 50.090%
Epoch   4 | Test Acc: 60.130%
Epoch   5 | Test Acc: 67.400%
Epoch   6 | Test Acc: 75.150%
Epoch   7 | Test Acc: 73.720%
Epoch   8 | Test Acc: 76.470%
Epoch   9 | Test Acc: 77.840%
Epoch  10 | Test Acc: 81.290%
Epoch  11 | Test Acc: 81.790%
Epoch  12 | Test Acc: 81.710%
Epoch  13 | Test Acc: 81.440%
Epoch  14 | Test Acc: 82.500%
Epoch  15 | Test Acc: 84.700%
Epoch  16 | Test Acc: 80.690%
Epoch  17 | Test Acc: 83.970%
Epoch  18 | Test Acc: 82.840%
Epoch  19 | Test Acc: 85.500%
Epoch  20 | Test Acc: 82.210%
Epoch  21 | Test Acc: 82.910%
Epoch  22 | Test Acc: 85.380%
Epoch  23 | Test Acc: 85.260%
Epoch  24 | Test Acc: 84.640%
Epoch  25 | Test Acc: 85.800%
Epoch  26 | Test Acc: 85.610%
Epoch  27 | Test Acc: 85.670%
Epoch  28 | Test Acc: 83.900%
Epoch  29 | Test Acc: 87.140%
Epoch  30 | Test Acc: 85.790%
Epoch  31 | Test Acc: 86.710%
Epoch  32 | Test Acc: 86.340%
Epoch  33 | Test Acc: 87.250%
Epoch  34 | Test Acc: 85.310%
Epoch  35 | Test Acc: 87.350%
Epoch  36 | Test Acc: 88.010%
Epoch  37 | Test Acc: 87.180%
Epoch  38 | Test Acc: 86.560%
Epoch  39 | Test Acc: 87.440%
Epoch  40 | Test Acc: 87.490%
Epoch  41 | Test Acc: 86.870%
Epoch  42 | Test Acc: 86.990%
Epoch  43 | Test Acc: 87.040%
Epoch  44 | Test Acc: 86.830%
Epoch  45 | Test Acc: 86.270%
Epoch  46 | Test Acc: 87.640%
Epoch  47 | Test Acc: 87.480%
Epoch  48 | Test Acc: 84.070%
Epoch  49 | Test Acc: 87.690%
Epoch  50 | Test Acc: 87.560%
Epoch  51 | Test Acc: 88.260%
Epoch  52 | Test Acc: 86.060%
Epoch  53 | Test Acc: 88.270%
Epoch  54 | Test Acc: 88.000%
Epoch  55 | Test Acc: 88.210%
Epoch  56 | Test Acc: 88.450%
Epoch  57 | Test Acc: 88.800%
Epoch  58 | Test Acc: 86.650%
Epoch  59 | Test Acc: 88.660%
Epoch  60 | Test Acc: 88.860%
Epoch  61 | Test Acc: 88.870%
Epoch  62 | Test Acc: 87.970%
Epoch  63 | Test Acc: 88.580%
Epoch  64 | Test Acc: 86.320%
Epoch  65 | Test Acc: 88.540%
Epoch  66 | Test Acc: 89.100%
Epoch  67 | Test Acc: 86.940%
Epoch  68 | Test Acc: 86.560%
Epoch  69 | Test Acc: 87.430%
Epoch  70 | Test Acc: 88.210%
Epoch  71 | Test Acc: 88.560%
Epoch  72 | Test Acc: 89.140%
Epoch  73 | Test Acc: 89.290%
Epoch  74 | Test Acc: 87.920%
Epoch  75 | Test Acc: 88.920%
Epoch  76 | Test Acc: 86.120%
Epoch  77 | Test Acc: 89.070%
Epoch  78 | Test Acc: 88.530%
Epoch  79 | Test Acc: 89.600%
Epoch  80 | Test Acc: 88.430%
Epoch  81 | Test Acc: 89.780%
Epoch  82 | Test Acc: 88.910%
Epoch  83 | Test Acc: 89.710%
Epoch  84 | Test Acc: 88.510%
Epoch  85 | Test Acc: 88.720%
Epoch  86 | Test Acc: 89.460%
Epoch  87 | Test Acc: 89.680%
Epoch  88 | Test Acc: 87.900%
Epoch  89 | Test Acc: 88.640%
Epoch  90 | Test Acc: 89.780%
Epoch  91 | Test Acc: 89.070%
Epoch  92 | Test Acc: 89.370%
Epoch  93 | Test Acc: 89.600%
Epoch  94 | Test Acc: 89.060%
Epoch  95 | Test Acc: 89.190%
Epoch  96 | Test Acc: 88.660%
Epoch  97 | Test Acc: 89.290%
Epoch  98 | Test Acc: 89.460%
Epoch  99 | Test Acc: 88.630%
Epoch 100 | Test Acc: 90.090%
Epoch 101 | Test Acc: 89.910%
Epoch 102 | Test Acc: 89.830%
Epoch 103 | Test Acc: 89.520%
Epoch 104 | Test Acc: 89.710%
Epoch 105 | Test Acc: 89.600%
Epoch 106 | Test Acc: 89.830%
Epoch 107 | Test Acc: 89.680%
Epoch 108 | Test Acc: 89.490%
Epoch 109 | Test Acc: 89.950%
Epoch 110 | Test Acc: 89.010%
Epoch 111 | Test Acc: 90.230%
Epoch 112 | Test Acc: 90.540%
Epoch 113 | Test Acc: 90.310%
Epoch 114 | Test Acc: 90.130%
Epoch 115 | Test Acc: 89.870%
Epoch 116 | Test Acc: 90.940%
Epoch 117 | Test Acc: 91.110%
Epoch 118 | Test Acc: 90.470%
Epoch 119 | Test Acc: 90.920%
Epoch 120 | Test Acc: 89.220%
Epoch 121 | Test Acc: 90.380%
Epoch 122 | Test Acc: 91.010%
Epoch 123 | Test Acc: 90.010%
Epoch 124 | Test Acc: 90.740%
Epoch 125 | Test Acc: 90.630%
Epoch 126 | Test Acc: 90.320%
Epoch 127 | Test Acc: 90.860%
Epoch 128 | Test Acc: 91.140%
Epoch 129 | Test Acc: 90.950%
Epoch 130 | Test Acc: 91.340%
Epoch 131 | Test Acc: 90.800%
Epoch 132 | Test Acc: 90.600%
Epoch 133 | Test Acc: 90.760%
Epoch 134 | Test Acc: 91.290%
Epoch 135 | Test Acc: 91.180%
Epoch 136 | Test Acc: 90.710%
Epoch 137 | Test Acc: 90.850%
Epoch 138 | Test Acc: 90.540%
Epoch 139 | Test Acc: 90.500%
Epoch 140 | Test Acc: 91.470%
Epoch 141 | Test Acc: 91.390%
Epoch 142 | Test Acc: 91.760%
Epoch 143 | Test Acc: 91.340%
Epoch 144 | Test Acc: 90.920%
Epoch 145 | Test Acc: 91.720%
Epoch 146 | Test Acc: 91.880%
Epoch 147 | Test Acc: 91.760%
Epoch 148 | Test Acc: 91.720%
Epoch 149 | Test Acc: 91.900%
Epoch 150 | Test Acc: 92.000%
Epoch 151 | Test Acc: 92.080%
Epoch 152 | Test Acc: 91.960%
Epoch 153 | Test Acc: 91.820%
Epoch 154 | Test Acc: 92.030%
Epoch 155 | Test Acc: 91.980%
Epoch 156 | Test Acc: 92.150%
Epoch 157 | Test Acc: 92.020%
Epoch 158 | Test Acc: 92.220%
Epoch 159 | Test Acc: 91.930%
Epoch 160 | Test Acc: 92.120%
Epoch 161 | Test Acc: 92.290%
Epoch 162 | Test Acc: 92.270%
Epoch 163 | Test Acc: 92.140%
Epoch 164 | Test Acc: 92.420%
Epoch 165 | Test Acc: 92.190%
Epoch 166 | Test Acc: 92.320%
Epoch 167 | Test Acc: 92.300%
Epoch 168 | Test Acc: 92.360%
Epoch 169 | Test Acc: 92.390%
Epoch 170 | Test Acc: 92.170%
Epoch 171 | Test Acc: 92.300%
Epoch 172 | Test Acc: 92.650%
Epoch 173 | Test Acc: 92.400%
Epoch 174 | Test Acc: 92.600%
Epoch 175 | Test Acc: 92.600%
Epoch 176 | Test Acc: 92.590%
Epoch 177 | Test Acc: 92.720%
Epoch 178 | Test Acc: 92.730%
Epoch 179 | Test Acc: 92.670%
Epoch 180 | Test Acc: 92.720%
Epoch 181 | Test Acc: 92.600%
Epoch 182 | Test Acc: 92.670%
Epoch 183 | Test Acc: 92.820%
Epoch 184 | Test Acc: 92.730%
Epoch 185 | Test Acc: 92.790%
Epoch 186 | Test Acc: 92.790%
Epoch 187 | Test Acc: 92.750%
Epoch 188 | Test Acc: 92.780%
Epoch 189 | Test Acc: 92.810%
Epoch 190 | Test Acc: 92.800%
Epoch 191 | Test Acc: 92.790%
Epoch 192 | Test Acc: 92.770%
Epoch 193 | Test Acc: 92.770%
Epoch 194 | Test Acc: 92.780%
Epoch 195 | Test Acc: 92.830%
Epoch 196 | Test Acc: 92.740%
Epoch 197 | Test Acc: 92.740%
Epoch 198 | Test Acc: 92.760%
Epoch 199 | Test Acc: 92.790%
Epoch 200 | Test Acc: 92.830%

Final Best Accuracy: 92.830%
Total training time: 83.59 minutes

Generating Plots..."""

import torch
import torch.nn as nn
import math
import torch.nn.functional as F
import torch.optim as optim
import time
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
!pip install fvcore # Install the missing library
from fvcore.nn import FlopCountAnalysis, parameter_count
import json

# --------------------------------------------------------
#  GHOST MODULE
# --------------------------------------------------------
class GhostModule(nn.Module):
    def __init__(self, in_channels, out_channels, primary_kernel_size=1, ratio=2, dw_kernel_size=3, stride=1):
        super(GhostModule, self).__init__()
        self.ratio = ratio
        self.out_channels = out_channels

        self.init_channels = math.ceil(out_channels / ratio)
        self.new_channels = self.init_channels * (self.ratio - 1)

        self.primary_conv = nn.Conv2d(
            in_channels,
            self.init_channels,
            kernel_size=primary_kernel_size,
            stride=stride,
            padding=(primary_kernel_size - 1) // 2,
            bias=False
        )

        self.cheap_operations = nn.Conv2d(
            in_channels=self.init_channels,
            out_channels=self.new_channels,
            kernel_size=dw_kernel_size,
            stride=1,
            padding=(dw_kernel_size - 1) // 2,
            groups=self.init_channels,
            bias=False
        )

    def forward(self, x):
        intrinsic = self.primary_conv(x)
        cheap = self.cheap_operations(intrinsic)
        out = torch.cat([intrinsic, cheap], dim=1)
        return out[:, :self.out_channels, :, :]


# --------------------------------------------------------
#  BASIC RESNET BLOCK
# --------------------------------------------------------
class BasicBlock(nn.Module):
    expansion = 1
    def __init__(self, in_planes, planes, stride=1):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, 3, stride, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, 3, 1, 1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, planes, 1, stride, bias=False),
                nn.BatchNorm2d(planes)
            )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        return F.relu(out)


# --------------------------------------------------------
#  RESNET-56
# --------------------------------------------------------
class ResNet(nn.Module):
    def __init__(self, block, num_blocks, num_classes=10):
        super(ResNet, self).__init__()
        self.in_planes = 16

        self.conv1 = nn.Conv2d(3, 16, 3, 1, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(16)

        self.layer1 = self._make_layer(block, 16, num_blocks[0], 1)
        self.layer2 = self._make_layer(block, 32, num_blocks[1], 2)
        self.layer3 = self._make_layer(block, 64, num_blocks[2], 2)

        self.linear = nn.Linear(64, num_classes)

    def _make_layer(self, block, planes, num_blocks, stride):
        layers = []
        layers.append(block(self.in_planes, planes, stride))
        self.in_planes = planes
        for _ in range(num_blocks - 1):
            layers.append(block(self.in_planes, planes))
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        # Modified: Use adaptive_avg_pool2d for robust global average pooling
        out = F.adaptive_avg_pool2d(out, 1)
        out = out.view(out.size(0), -1)
        return self.linear(out)

def ResNet56():
    return ResNet(BasicBlock, [9, 9, 9])


# --------------------------------------------------------
#  REPLACE CONV2D WITH GHOST MODULES
# --------------------------------------------------------
def convert_to_ghost(model, ratio=2, dw_kernel_size=3):
    for name, module in model.named_children():
        if isinstance(module, nn.Conv2d):
            ghost = GhostModule(
                module.in_channels,
                module.out_channels,
                primary_kernel_size=module.kernel_size[0],
                ratio=ratio,
                dw_kernel_size=dw_kernel_size,
                stride=module.stride[0]
            )
            setattr(model, name, ghost)

        elif len(list(module.children())) > 0:
            convert_to_ghost(module, ratio, dw_kernel_size)


# --------------------------------------------------------
#  FLOPS AND PARAM COUNT
# --------------------------------------------------------
def compute_flops(model, device):
    model.eval()
    dummy = torch.randn(1, 3, 32, 32).to(device)
    flops = FlopCountAnalysis(model, dummy).total()
    params = parameter_count(model)['']
    return flops, params

def print_stats(model, device):
    flops, params = compute_flops(model, device)
    print("\n========== MODEL COMPLEXITY ==========")
    print(f"FLOPs : {flops/1e6:.2f} MFLOPs")
    print(f"Params: {params/1e6:.2f} M")
    print("======================================\n")
    return flops, params


# --------------------------------------------------------
#  BUILD DATASET
# --------------------------------------------------------
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010)),
])
transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010)),
])

batch_size = 128
trainloader = torch.utils.data.DataLoader(
    torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train),
    batch_size=batch_size, shuffle=True, num_workers=2
)
testloader = torch.utils.data.DataLoader(
    torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test),
    batch_size=batch_size, shuffle=False, num_workers=2
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# --------------------------------------------------------
#  TRAIN + TEST FUNCTIONS
# --------------------------------------------------------
def train_fn(model, optimizer, criterion):
    model.train()
    for x, y in trainloader:
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()
        pred = model(x)
        loss = criterion(pred, y)
        loss.backward()
        optimizer.step()

def test_fn(model, criterion):
    model.eval()
    correct = total = 0
    with torch.no_grad():
        for x, y in testloader:
            x, y = x.to(device), y.to(device)
            pred = model(x)
            _, p = pred.max(1)
            correct += p.eq(y).sum().item()
            total += y.size(0)
    return 100 * correct / total


# --------------------------------------------------------
#  TRAIN BOTH MODELS (VANILLA + GHOST)
# --------------------------------------------------------
EPOCHS = 200

# VANILLA
vanilla = ResNet56().to(device)
optim_v = optim.SGD(vanilla.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)
sched_v = optim.lr_scheduler.CosineAnnealingLR(optim_v, T_max=EPOCHS)
crit = nn.CrossEntropyLoss()

vanilla_flops, vanilla_params = print_stats(vanilla, device)
vanilla_acc_log = []

for ep in range(EPOCHS):
    train_fn(vanilla, optim_v, crit)
    acc = test_fn(vanilla, crit)
    vanilla_acc_log.append(acc)
    sched_v.step()
    print(f"[Vanilla] Epoch {ep} Acc = {acc:.2f}%")

# GHOST
ghost = ResNet56()
convert_to_ghost(ghost)
ghost = ghost.to(device)

optim_g = optim.SGD(ghost.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)
sched_g = optim.lr_scheduler.CosineAnnealingLR(optim_g, T_max=EPOCHS)

ghost_flops, ghost_params = print_stats(ghost, device)
ghost_acc_log = []

for ep in range(EPOCHS):
    train_fn(ghost, optim_g, crit)
    acc = test_fn(ghost, crit)
    ghost_acc_log.append(acc)
    sched_g.step()
    print(f"[Ghost] Epoch {ep} Acc = {acc:.2f}%")


# --------------------------------------------------------
#  SAVE RESULTS
# --------------------------------------------------------
results = {
    "vanilla_acc": vanilla_acc_log,
    "ghost_acc": ghost_acc_log,
    "vanilla_flops": vanilla_flops,
    "ghost_flops": ghost_flops,
    "vanilla_params": vanilla_params,
    "ghost_params": ghost_params
}
with open("results.json", "w") as f:
    json.dump(results, f, indent=4)


# --------------------------------------------------------
#  GRAPHS
# --------------------------------------------------------

# 1. Accuracy curves
plt.figure(figsize=(10,5))
plt.plot(vanilla_acc_log, label="Vanilla ResNet56")
plt.plot(ghost_acc_log, label="Ghost-ResNet56")
plt.title("Accuracy vs Epochs")
plt.xlabel("Epoch")
plt.ylabel("Accuracy (%)")
plt.legend()
plt.grid()
plt.savefig("accuracy_curves.png")
plt.show()

# 2. FLOPs vs Accuracy (final)
plt.figure(figsize=(7,5))
plt.scatter(vanilla_flops/1e6, vanilla_acc_log[-1], s=120, label="Vanilla")
plt.scatter(ghost_flops/1e6, ghost_acc_log[-1], s=120, label="Ghost")
plt.title("FLOPs vs Final Accuracy")
plt.xlabel("MFLOPs")
plt.ylabel("Accuracy (%)")
plt.legend()
plt.grid()
plt.savefig("flops_vs_accuracy.png")
plt.show()

# 3. Paper vs Your Results (Accuracy)
ghostnet_paper_acc = 75.7
plt.figure(figsize=(7,5))
plt.bar(["GhostNet Paper", "Your Ghost-ResNet56"],
        [ghostnet_paper_acc, ghost_acc_log[-1]])
plt.ylabel("Accuracy (%)")
plt.title("Accuracy Comparison: Paper vs Your GhostNet")
plt.savefig("paper_vs_your_accuracy.png")
plt.show()

# 4. Paper vs Your Results (FLOPs)
ghostnet_paper_flops = 141
plt.figure(figsize=(7,5))
plt.bar(["GhostNet Paper", "Your Ghost-ResNet56"],
        [ghostnet_paper_flops, ghost_flops/1e6])
plt.ylabel("MFLOPs")
plt.title("FLOPs Comparison: Paper vs Your GhostNet")
plt.savefig("paper_vs_your_flops.png")
plt.show()

# 5. Ghost vs Vanilla FLOPs
plt.figure(figsize=(7,5))
plt.bar(["Vanilla ResNet56", "Ghost-ResNet56"],
        [vanilla_flops/1e6, ghost_flops/1e6])
plt.ylabel("MFLOPs")
plt.title("FLOPs: Ghost-ResNet56 vs Vanilla ResNet56")
plt.savefig("vanilla_vs_ghost_flops.png")
plt.show()

"""========== MODEL COMPLEXITY ==========
FLOPs : 126.84 MFLOPs
Params: 0.86 M
======================================

[Vanilla] Epoch 0 Acc = 36.70%
[Vanilla] Epoch 1 Acc = 42.97%
[Vanilla] Epoch 2 Acc = 51.62%
[Vanilla] Epoch 3 Acc = 62.04%
[Vanilla] Epoch 4 Acc = 67.19%
[Vanilla] Epoch 5 Acc = 72.26%
[Vanilla] Epoch 6 Acc = 75.98%
[Vanilla] Epoch 7 Acc = 74.82%
[Vanilla] Epoch 8 Acc = 80.74%
[Vanilla] Epoch 9 Acc = 79.48%
[Vanilla] Epoch 10 Acc = 81.39%
[Vanilla] Epoch 11 Acc = 82.83%
[Vanilla] Epoch 12 Acc = 81.77%
[Vanilla] Epoch 13 Acc = 81.21%
[Vanilla] Epoch 14 Acc = 83.07%
[Vanilla] Epoch 15 Acc = 83.40%
[Vanilla] Epoch 16 Acc = 82.97%
[Vanilla] Epoch 17 Acc = 83.93%
[Vanilla] Epoch 18 Acc = 81.93%
[Vanilla] Epoch 19 Acc = 84.92%
[Vanilla] Epoch 20 Acc = 84.21%
[Vanilla] Epoch 21 Acc = 83.90%
[Vanilla] Epoch 22 Acc = 83.86%
[Vanilla] Epoch 23 Acc = 84.99%
[Vanilla] Epoch 24 Acc = 82.16%
[Vanilla] Epoch 25 Acc = 84.13%
[Vanilla] Epoch 26 Acc = 83.97%
[Vanilla] Epoch 27 Acc = 85.11%
[Vanilla] Epoch 28 Acc = 84.99%
[Vanilla] Epoch 29 Acc = 84.52%
[Vanilla] Epoch 30 Acc = 86.44%
[Vanilla] Epoch 31 Acc = 85.96%
[Vanilla] Epoch 32 Acc = 86.80%
[Vanilla] Epoch 33 Acc = 86.47%
[Vanilla] Epoch 34 Acc = 85.67%
[Vanilla] Epoch 35 Acc = 86.66%
[Vanilla] Epoch 36 Acc = 86.50%
[Vanilla] Epoch 37 Acc = 86.91%
[Vanilla] Epoch 38 Acc = 86.49%
[Vanilla] Epoch 39 Acc = 86.19%
[Vanilla] Epoch 40 Acc = 83.95%
[Vanilla] Epoch 41 Acc = 87.99%
[Vanilla] Epoch 42 Acc = 85.86%
[Vanilla] Epoch 43 Acc = 87.02%
[Vanilla] Epoch 44 Acc = 87.20%
[Vanilla] Epoch 45 Acc = 86.12%
[Vanilla] Epoch 46 Acc = 87.61%
[Vanilla] Epoch 47 Acc = 87.44%
[Vanilla] Epoch 48 Acc = 86.90%
[Vanilla] Epoch 49 Acc = 85.73%
[Vanilla] Epoch 50 Acc = 86.43%
[Vanilla] Epoch 51 Acc = 86.60%
[Vanilla] Epoch 52 Acc = 87.45%
[Vanilla] Epoch 53 Acc = 87.48%
[Vanilla] Epoch 54 Acc = 87.66%
[Vanilla] Epoch 55 Acc = 88.82%
[Vanilla] Epoch 56 Acc = 87.68%
[Vanilla] Epoch 57 Acc = 88.30%
[Vanilla] Epoch 58 Acc = 88.14%
[Vanilla] Epoch 59 Acc = 88.70%
[Vanilla] Epoch 60 Acc = 88.03%
[Vanilla] Epoch 61 Acc = 88.43%
[Vanilla] Epoch 62 Acc = 88.14%
[Vanilla] Epoch 63 Acc = 87.94%
[Vanilla] Epoch 64 Acc = 87.33%
[Vanilla] Epoch 65 Acc = 89.43%
[Vanilla] Epoch 66 Acc = 87.96%
[Vanilla] Epoch 67 Acc = 88.45%
[Vanilla] Epoch 68 Acc = 88.12%
[Vanilla] Epoch 69 Acc = 90.04%
[Vanilla] Epoch 70 Acc = 87.65%
[Vanilla] Epoch 71 Acc = 89.07%
[Vanilla] Epoch 72 Acc = 89.22%
[Vanilla] Epoch 73 Acc = 88.84%
[Vanilla] Epoch 74 Acc = 89.36%
[Vanilla] Epoch 75 Acc = 89.41%
[Vanilla] Epoch 76 Acc = 87.89%
[Vanilla] Epoch 77 Acc = 86.86%
[Vanilla] Epoch 78 Acc = 89.13%
[Vanilla] Epoch 79 Acc = 88.47%
[Vanilla] Epoch 80 Acc = 89.01%
[Vanilla] Epoch 81 Acc = 86.71%
[Vanilla] Epoch 82 Acc = 89.15%
[Vanilla] Epoch 83 Acc = 89.39%
[Vanilla] Epoch 84 Acc = 89.93%
[Vanilla] Epoch 85 Acc = 88.93%
[Vanilla] Epoch 86 Acc = 88.55%
[Vanilla] Epoch 87 Acc = 88.42%
[Vanilla] Epoch 88 Acc = 90.06%
[Vanilla] Epoch 89 Acc = 90.04%
[Vanilla] Epoch 90 Acc = 89.05%
[Vanilla] Epoch 91 Acc = 88.87%
[Vanilla] Epoch 92 Acc = 88.18%
[Vanilla] Epoch 93 Acc = 89.56%
[Vanilla] Epoch 94 Acc = 88.57%
[Vanilla] Epoch 95 Acc = 89.25%
[Vanilla] Epoch 96 Acc = 88.41%
[Vanilla] Epoch 97 Acc = 90.09%
[Vanilla] Epoch 98 Acc = 89.92%
[Vanilla] Epoch 99 Acc = 89.44%
[Vanilla] Epoch 100 Acc = 88.94%
[Vanilla] Epoch 101 Acc = 89.89%
[Vanilla] Epoch 102 Acc = 89.48%
[Vanilla] Epoch 103 Acc = 89.48%
[Vanilla] Epoch 104 Acc = 90.40%
[Vanilla] Epoch 105 Acc = 90.14%
[Vanilla] Epoch 106 Acc = 89.25%
[Vanilla] Epoch 107 Acc = 89.78%
[Vanilla] Epoch 108 Acc = 90.02%
[Vanilla] Epoch 109 Acc = 89.60%
[Vanilla] Epoch 110 Acc = 90.48%
[Vanilla] Epoch 111 Acc = 90.74%
[Vanilla] Epoch 112 Acc = 90.45%
[Vanilla] Epoch 113 Acc = 89.99%
[Vanilla] Epoch 114 Acc = 89.40%
[Vanilla] Epoch 115 Acc = 90.74%
[Vanilla] Epoch 116 Acc = 90.63%
[Vanilla] Epoch 117 Acc = 91.12%
[Vanilla] Epoch 118 Acc = 90.51%
[Vanilla] Epoch 119 Acc = 90.78%
[Vanilla] Epoch 120 Acc = 90.68%
[Vanilla] Epoch 121 Acc = 90.46%
[Vanilla] Epoch 122 Acc = 90.98%
[Vanilla] Epoch 123 Acc = 91.13%
[Vanilla] Epoch 124 Acc = 90.94%
[Vanilla] Epoch 125 Acc = 91.22%
[Vanilla] Epoch 126 Acc = 90.78%
[Vanilla] Epoch 127 Acc = 91.13%
[Vanilla] Epoch 128 Acc = 91.01%
[Vanilla] Epoch 129 Acc = 91.25%
[Vanilla] Epoch 130 Acc = 91.18%
[Vanilla] Epoch 131 Acc = 91.65%
[Vanilla] Epoch 132 Acc = 91.19%
[Vanilla] Epoch 133 Acc = 91.75%
[Vanilla] Epoch 134 Acc = 90.69%
[Vanilla] Epoch 135 Acc = 91.04%
[Vanilla] Epoch 136 Acc = 91.73%
[Vanilla] Epoch 137 Acc = 91.91%
[Vanilla] Epoch 138 Acc = 91.84%
[Vanilla] Epoch 139 Acc = 91.90%
[Vanilla] Epoch 140 Acc = 92.26%
[Vanilla] Epoch 141 Acc = 91.96%
[Vanilla] Epoch 142 Acc = 92.47%
[Vanilla] Epoch 143 Acc = 92.30%
[Vanilla] Epoch 144 Acc = 92.20%
[Vanilla] Epoch 145 Acc = 92.39%
[Vanilla] Epoch 146 Acc = 92.29%
[Vanilla] Epoch 147 Acc = 92.57%
[Vanilla] Epoch 148 Acc = 92.32%
[Vanilla] Epoch 149 Acc = 92.69%
[Vanilla] Epoch 150 Acc = 92.38%
[Vanilla] Epoch 151 Acc = 92.74%
[Vanilla] Epoch 152 Acc = 92.41%
[Vanilla] Epoch 153 Acc = 92.75%
[Vanilla] Epoch 154 Acc = 92.94%
[Vanilla] Epoch 155 Acc = 92.90%
[Vanilla] Epoch 156 Acc = 93.06%
[Vanilla] Epoch 157 Acc = 92.98%
[Vanilla] Epoch 158 Acc = 93.15%
[Vanilla] Epoch 159 Acc = 92.83%
[Vanilla] Epoch 160 Acc = 93.00%
[Vanilla] Epoch 161 Acc = 92.90%"""